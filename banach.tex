\documentclass{beamer}
\usetheme{metropolis}           % Use metropolis theme
\title{Generative Adversarial Nets}
\date{\today}
\author{Kacper}
\institute{Paper by an J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozairy, Aaron Courville, Yoshua Bengio}
\begin{document}
  \maketitle
  
  
  
\begin{frame}{Learning density}
The aim is to learn distribution $p$ on a Polish space.

\pause 
{\tiny
 Omg some Polish space, will be sooooooooooooooo boring ...   
 
\pause
 Can we consider more general spaces?}
\pause

We want to  learn a generative model $G$, such that for a random variable $Z$, $G(Z) \sim p$.

Or, $p = p_G$, where $p_G$ is distribution of $G(Z)$.
\end{frame}
  
  
\begin{frame}{artificial  labels}
  \begin{itemize}
   \item Random variable  $X$, distributed according to distribution $p$. 
   \item Some noise random variable $Z$.
   \item A generative model $G$, which transforms $Z$ to $X' = G(Z)$, in other words  $X' \sim p_{G}$.
\end{itemize}

Consider an artificial  label   $Y \in \{ -1,1 \}$, distributed according to Bernoulli distribution. 
Consider a random vector $W$, such that $W_1 = Y$.  

if $Y=1$  then  $W_2 =X \sim p$.

if $Y=-1$  then $W_2 =X' \sim p_G$.

I probably could have written $W  = (1,X) | (-1,X')$.

\end{frame}
  
  
   
\begin{frame}{Classifier}

Let $D$ be discriminative function which is suppose to figure out if $W_2$ comes from $p$ or $p_G$ (if $W_2 = X'$ or $W_2=X$). 

For a random variable  $V$,  $D(W_2)$ a probability that $W_1 =1$ (label is one).  

\end{frame}
  
  
  
  
\begin{frame}{Learning density}

Consider an expression  
$$
E \left( \log(D(W_2)) | Y=1 \right)  + E \left( \log(1 - D(W_2))| Y=-1  \right).
$$
It can be written using $X$,$G(Z)$, as a function of $D,G$
$$
V(D,G) = E \log(D(X)) + E \log(1 - D(G(Z)))
$$
For each generator  $G$ there exists an optimal  discriminator $D$ that maximizes the above function (we will show). 
$$C(G) = \sup_{D} V(D,G)$$

We are going to show that $G$ that minimizes $C(G)$ is the best generator, i.e. $p_G = p$.
\end{frame}
  
\begin{frame}{Optimal discriminator}
For each generator  $G$ there exists an optimal  discriminator $D$ that maximizes the above function (as we will show).
\begin{align}
V(D,G) =& E \left[ \log D(X) +  \log(1 - D(G(Z)))  \right] \\ 
       =& E\left[  \log D(X) +  \log(1 - D(X')) \right] \\
       =& \int p(x)  \log D(x) +  p_G(x) \log(1 - D(x))
\end{align}
which, for a fixed $G$, $V(D,G)$ reaches  maximum  at $D = \frac{p}{p+p_g}$


\end{frame}
  
  
\begin{frame}{Optimal generator }
For a fixed, optimal $D = \frac{p}{p+p_g} $, $V$ is a divergence 
\begin{align}
V(D,G) =& E \left(  \log D(X) + \log(1 - D(X')) \right) \\ 
&E \left(   \log \frac{p(X)}{p(X)+p_g(X)} + \log \frac{p_g(X')}{p(X')+p_g(X')} \right) =\\
&D_{KL}(p || \frac{p+p_g}{2} ) + D_{KL}(p_g ||  \frac{p+p_g}{2})
\end{align}

\end{frame}  
  
\end{document}